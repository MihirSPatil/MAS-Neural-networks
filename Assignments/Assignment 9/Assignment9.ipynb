{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "* Erick Kramer\n",
    "* Swaroop Bhandary\n",
    "* Mihir Patil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions: \n",
    "Implement the neural image captioning model (NIC) as described previously in the lectures for the the IAPR TC-12 dataset.\n",
    "\n",
    "Dataset can be found here: https://www.imageclef.org/photodata\n",
    "\n",
    "Original paper can be found here: https://arxiv.org/abs/1411.4555\n",
    "\n",
    "EVALUATION:\n",
    "\n",
    "- Data manager class  for loading dataset.\n",
    "\n",
    "- The dataset to be loaded should be a dictionary including as keys the image path and as respective values the description of that image (this alleviates the memory resources).\n",
    "\n",
    "- Generator class that takes the data dictionary constructed by the data manager class. This class generates batches for the network.\n",
    "\n",
    "- NIC model.\n",
    "\n",
    "- Display of training loss and training accuracy.\n",
    "\n",
    "- Display descriptions generated by the validation/test images.\n",
    "\n",
    "- Display of random images taken from the internet. \n",
    "\n",
    "TIPS:\n",
    "\n",
    "- REUSE your code from the previous exercise that included generation of text. \n",
    "\n",
    "- Base your data manager solution from: https://github.com/oarriaga/neural_image_captioning\n",
    "\n",
    "- Give proper credit when using explicitly code of others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import os\n",
    "import pickle\n",
    "from string import digits\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np \n",
    "import pandas as np\n",
    "\n",
    "class DataManager(object):\n",
    "    '''\n",
    "    Class to manage and preprocess data. \n",
    "    \n",
    "    # Disclaimer: \n",
    "    This class was developed based on the code presented in: https://github.com/oarriaga/neural_image_captioning/blob/master/src/data_manager.py\n",
    "    \n",
    "    # Arguments:\n",
    "    * data_filename: File containing in every row the caption and \n",
    "    the image name, separated by a special character, i.e. separator\n",
    "    * extract_image_features: Flag to create a h5py file that contains\n",
    "    a vector of features extracted by a pre-trained CNN \n",
    "    * image_directory: Path to the images to extract the features    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data_filename, max_caption_length=20., separator=\"*\",\n",
    "                word_frequency_threshold=2, randomize_data=True,\n",
    "                split_data_flag=True, extract_image_features=False,\n",
    "                image_directory=None, dump_path='preprocessed_data',\n",
    "                extractor_model='inception'):\n",
    "        self.data_filename            = data_filename \n",
    "        self.max_caption_length       = max_caption_length\n",
    "        self.separator                = separator\n",
    "        self.word_frequency_threshold = word_frequency_threshold\n",
    "        self.randomize_data           = randomize_data\n",
    "        self.split_data_flag          = split_data_flag\n",
    "        self.extract_image_features   = extract_image_features\n",
    "        self.image_directory          = image_directory\n",
    "        self.dump_path                = dump_path\n",
    "        self.extractor_model          = extractor_model\n",
    "               \n",
    "        self.current_directory = os.getcwd()\n",
    "        # Tag format \n",
    "        self.BOS = '<S>' # Beginning of sentence\n",
    "        self.EOS = '<E>' # End of sentence\n",
    "        self.PAD = '<P>' # TODO: Define\n",
    "        \n",
    "        self.word_frequencies = None\n",
    "        self.captions = None\n",
    "        self.image_files = None\n",
    "        self.image_features = None\n",
    "        self.word_to_id = None\n",
    "        self.id_to_word = None\n",
    "        self.extracted_features = None\n",
    "        self.features_file_names = None\n",
    "        self.image_feature_files = None\n",
    "        self.elapsed_time = None\n",
    "        \n",
    "        # To extract image features assert directory is given\n",
    "        if self.extract_image_features:\n",
    "            assert self.image_directory != None\n",
    "        \n",
    "        def preprocess(self):\n",
    "            '''\n",
    "            Preprocessing function does the following:\n",
    "            * remove long captions\n",
    "            * get dataset statistics\n",
    "            * remove words with frequency below the threshold\n",
    "            * create image dictionaries\n",
    "            * get image features (if requested)\n",
    "            '''\n",
    "            start_time = time.monotonic() # Clock not affected by system  clock updates\n",
    "            self.load(self.data_filename)\n",
    "            self.remove_long_captions()\n",
    "            self.get_corpus_statistics()\n",
    "            self.remove_infrequent_words()\n",
    "            self.construct_dictionaries()\n",
    "            if self.extract_image_features:\n",
    "                self.get_image_features(self.image_directory)\n",
    "                self.move_to_path()\n",
    "                self.write_image_features_to_h5()\n",
    "            self.move_to_path()\n",
    "            self.write_data()\n",
    "            self.write_dictionaries()\n",
    "            self.elapsed_time = time.monotonic() - start_time\n",
    "            self.write_parameters()\n",
    "            if self.split_data_flag:\n",
    "                self.split_data()\n",
    "            self.move_path_back()\n",
    "            \n",
    "        def load(self, data_filename):\n",
    "            '''\n",
    "            Loads the image dataset\n",
    "            '''\n",
    "            data = pd.read_table(data_filename, sep=self.separator)\n",
    "            data = np.assarray(data)\n",
    "            if self.randomize_data:\n",
    "                np.random.shuffle(data)\n",
    "            self.image_files = data[:,0]\n",
    "            self.captions = data[:,1]\n",
    "            num_captions = self.image_files.shape[0]\n",
    "            print('Loaded dataset with ', num_captions,' images')\n",
    "            \n",
    "        def remove_long_captions(self):\n",
    "            '''\n",
    "            Remove captions that contains more than Max number\n",
    "            of characters\n",
    "            '''\n",
    "            print('Removing captions with more than ', \n",
    "                  self.max_caption_length, ' characters')\n",
    "            \n",
    "            kept_image_files = []\n",
    "            kept_captions = []\n",
    "            original_file_size = len(self.captions)\n",
    "            \n",
    "            for caption_idx, caption in enumerate(self.captions):\n",
    "                filtered_caption = self.filter_caption(caption)\n",
    "                if (len(filtered_caption) <= self.max_caption_length):\n",
    "                    kept_captions.append(filtered_caption)\n",
    "                    kept_image_files.append(self.image_files[caption_idx])\n",
    "                    \n",
    "            self.captions = kept_captions\n",
    "            self.image_files = kept_image_files\n",
    "        \n",
    "        def filter_caption(self, caption):\n",
    "            '''\n",
    "            Function that removes undesired characters, lowercase sentence,\n",
    "            remove quotes, and returns sentence as a list of words.\n",
    "            '''\n",
    "            undesired_chars = digits + \";.,'/*?Â¿><:{}[\\]|+\"\n",
    "            char_translator = str.maketrans('', '', undesired_chars)\n",
    "            quotes_translator = str.maketrans('','','\"')\n",
    "            filtered_caption = caption.strip().lower() #lowercase\n",
    "            filtered_caption = filtered_caption.translate(char_translator)\n",
    "            filtered_caption = filtered_caption.translate(quotes_translator)\n",
    "            filtered_caption = filtered_caption.split(' ') # list of words\n",
    "            return filtered_caption\n",
    "        \n",
    "        def get_corpus_statistics(self):\n",
    "            '''\n",
    "            Compute word frequencies\n",
    "            '''\n",
    "            self.word_frequencies = Counter(chain(*self.captions))\\\n",
    "            .most_common()\n",
    "            \n",
    "        def remove_sparse_words(self):\n",
    "            '''\n",
    "            Remove words with a frequency below a treshold\n",
    "            '''\n",
    "            print('Removing words with a frequency less than ',\n",
    "                 self.word_frequency_threshold)\n",
    "            \n",
    "            # PENDING\n",
    "        \n",
    "        def construct_dictionaries(self):\n",
    "            '''\n",
    "            Computes words_id, and id_words dictionaries\n",
    "            '''\n",
    "            words = self.word_frequencies[:,0]\n",
    "            self.word_to_id{self.PAD:0, self.BOS:1, self.EOS:2}\n",
    "            self.word_to_id.update({word:word_id for word_id, word in\n",
    "                                   enumerate(words, 3)})\n",
    "            \n",
    "            self.id_to_word = {word_id:word for word, word_id in\n",
    "                                   self.word_to_id.items()}\n",
    "        def get_image_features(self, image_directory):\n",
    "            \n",
    "            from keras.preprocesssing import image\n",
    "            from keras.models import Model\n",
    "            \n",
    "            if self.extractor_model == 'vgg16':\n",
    "                from keras.applications.vgg16 import preprocess_input\n",
    "                from keras.applications import VGG16\n",
    "                \n",
    "                self.IMG_FEATURES = 4096\n",
    "                base_model = VGG16(weights='imagenet')\n",
    "                model = Model(input=base_model.input,\n",
    "                              output=base_model.get_layer('fc2').output)\n",
    "                self.extract_features = []\n",
    "                self.image_feature_files = list(set(self.image_files))\n",
    "                num_images = len(self.image_feature_files)\n",
    "                \n",
    "                for image_idx, image_file in enumerate(self.image_feature_files):\n",
    "                    image_path = image_directory + image_file # Not sure if dir needed\n",
    "                    if image_idx%100 == 0:\n",
    "                    print('%.2f %% completed' %\n",
    "                            round(100*image_idx/num_images,2))\n",
    "                    img = image.load_img(image_path, target_size=(224,224))\n",
    "                    img = image.img_to_array(img)\n",
    "                    img = np.expand_dims(img, axis=0)\n",
    "                    img = preprocess_input(img)\n",
    "                    CNN_features = model.predict(img)\n",
    "                    self.extracted_features.append(np.squeeze(CNN_features))\n",
    "                self.extracted_features = np.asarray(self.extracted_features)\n",
    "                \n",
    "            elif self.extractor_model == 'vgg19':\n",
    "                from keras.applications.vgg19 import preprocess_input\n",
    "                from keras.applications import VGG19\n",
    "                \n",
    "                self.IMG_FEATURES = 4096\n",
    "                base_model = VGG19(weights='imagenet')\n",
    "                model = Model(input=base_model.input,\n",
    "                              output=base_model.get_layer('fc2').output)\n",
    "                self.extract_features = []\n",
    "                self.image_feature_files = list(set(self.image_files))\n",
    "                num_images = len(self.image_feature_files)\n",
    "                \n",
    "                for image_idx, image_file in enumerate(self.image_feature_files):\n",
    "                    image_path = image_directory + image_file # Not sure if dir needed\n",
    "                    if image_idx%100 == 0:\n",
    "                    print('%.2f %% completed' %\n",
    "                            round(100*image_idx/num_images,2))\n",
    "                    img = image.load_img(image_path, target_size=(224,224))\n",
    "                    img = image.img_to_array(img)\n",
    "                    img = np.expand_dims(img, axis=0)\n",
    "                    img = preprocess_input(img)\n",
    "                    CNN_features = model.predict(img)\n",
    "                    self.extracted_features.append(np.squeeze(CNN_features))\n",
    "                self.extracted_features = np.asarray(self.extracted_features)\n",
    "                \n",
    "            elif self.extractor_model == 'inception':\n",
    "                from keras.applications.inception_v3 import preprocess_input\n",
    "                from keras.applications import InceptionV3\n",
    "                \n",
    "                self.IMG_FEATURES = 2048\n",
    "                base_model = InceptionV3(weights='imagenet')\n",
    "                model = Model(input=base_model.input,\n",
    "                              output=base_model.get_layer('flatten').output)\n",
    "                self.extract_features = []\n",
    "                self.image_feature_files = list(set(self.image_files))\n",
    "                num_images = len(self.image_feature_files)\n",
    "                \n",
    "                for image_idx, image_file in enumerate(self.image_feature_files):\n",
    "                    image_path = image_directory + image_file # Not sure if dir needed\n",
    "                    if image_idx%100 == 0:\n",
    "                    print('%.2f %% completed' %\n",
    "                            round(100*image_idx/num_images,2))\n",
    "                    img = image.load_img(image_path, target_size=(224,224))\n",
    "                    img = image.img_to_array(img)\n",
    "                    img = np.expand_dims(img, axis=0)\n",
    "                    img = preprocess_input(img)\n",
    "                    CNN_features = model.predict(img)\n",
    "                    self.extracted_features.append(np.squeeze(CNN_features))\n",
    "                self.extracted_features = np.asarray(self.extracted_features)\n",
    "\n",
    "        def write_image_features_to_h5(self):\n",
    "            '''\n",
    "            Save the features in a h5 file\n",
    "            '''\n",
    "            dataset_file = h5py.File(self.extractor_model + \n",
    "                                    '_image_name_to_feature.h5')\n",
    "            num_features = len(self.image_feature_files)\n",
    "            for image_idx, image_file in enumerate(self.image_feature_files):\n",
    "                file_id = dataset_file.create_group(image_file)\n",
    "                image_data = file_id.create_dataset('image_features',\n",
    "                                                   (self.IMG_FEATURES,), dtype='float32')\n",
    "                image_data[:] = self.extracted_features[image_idx]\n",
    "                \n",
    "                if image_idx%100 == 0:\n",
    "                    print('Number of image processed: ', image_idx)\n",
    "                    print('Number of image remaining: ',\n",
    "                          num_features - image_idx)\n",
    "            dataset_file.close()\n",
    "            \n",
    "        def write_image_feature_files(self):\n",
    "            pickle.dump(self.image_feature_files,\n",
    "                       open('image_feature_files.p', 'wb'))\n",
    "        \n",
    "        def write_dictionaries(self):\n",
    "            '''\n",
    "            Save dictionaries in pickle files\n",
    "            '''\n",
    "            pickle.dump(self.word_to_id, open('word_to_id.p', 'wb'))\n",
    "            pickle.dump(self.id_to_word, open('id_to_word.p', 'wb'))\n",
    "        \n",
    "        def write_image_features(self):\n",
    "            pickle.dump(self.extracted_features,\n",
    "                       open('extracted_features.p', 'wb'))\n",
    "        \n",
    "        def write_parameters(self):\n",
    "            log_file = open('data_parameters.log','w')\n",
    "            log_file.write('data_filename %s \\n' %self.data_filename)\n",
    "            log_file.write('dump_path %s \\n' %self.dump_path)\n",
    "            log_file.write('BOS: %s \\n' % self.BOS)\n",
    "            log_file.write('EOS: %s \\n' % self.EOS)\n",
    "            log_file.write('PAD: %s \\n' % self.PAD)\n",
    "            log_file.write('IMG_FEATS: %s \\n' %self.IMG_FEATURES)\n",
    "            log_file.write('word_frequency_threshold: %s \\n'\n",
    "                        %self.word_frequency_treshold)\n",
    "            log_file.write('max_caption_length: %s \\n'\n",
    "                        %self.max_caption_length)\n",
    "            log_file.write('initial_data_size: %s \\n'\n",
    "                        %self.initial_number_of_captions)\n",
    "            log_file.write('captions_larger_than_threshold: %s \\n'\n",
    "                        %self.number_of_captions_removed)\n",
    "            log_file.write('current_data_size: %s \\n'\n",
    "                        %self.current_number_of_captions)\n",
    "            log_file.write('initial_word_size: %s \\n'\n",
    "                        %self.initial_number_of_words)\n",
    "            log_file.write('words_removed_by_frequency_threshold %s \\n'\n",
    "                        %self.number_of_words_removed)\n",
    "            log_file.write('current_word_size: %s \\n'\n",
    "                        %self.current_number_of_words)\n",
    "            log_file.write('cnn_extractor: %s \\n' %self.extractor_model)\n",
    "            log_file.write('elapsed_time: %s' %self.elapsed_time)\n",
    "            log_file.close()\n",
    "        \n",
    "        def split_data(self, train_porcentage=.80):\n",
    "\n",
    "            complete_data = pd.read_table('iaprtc12_images_annotations.txt',sep='*')\n",
    "            data_size = complete_data.shape[0]\n",
    "            training_size = int(data_size*train_porcentage)\n",
    "            complete_training_data = complete_data[0:training_size]\n",
    "            test_data = complete_data[training_size:]\n",
    "            test_data.to_csv('test_data.txt',sep='*',index=False)\n",
    "            # splitting between validation and training \n",
    "            training_size = int(training_size*train_porcentage)\n",
    "            validation_data = complete_training_data[training_size:]\n",
    "            training_data = complete_training_data[0:training_size]\n",
    "            validation_data.to_csv('validation_data.txt',sep='*',index=False)\n",
    "            training_data.to_csv('training_data.txt',sep='*',index=False)\n",
    "                \n",
    "                \n",
    "                \n",
    "                        \n",
    "                    \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.maketrans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'iaprtc12_images_annotations.txt' \n",
    "\n",
    "# Get image filenames\n",
    "images = []\n",
    "for root, dirs, files in os.walk('iaprtc12/images/'):\n",
    "    for name in files:\n",
    "        images.append(os.path.join(root,name))\n",
    "images = sorted(images) \n",
    "\n",
    "# Get annotations filenames\n",
    "annotations = []\n",
    "for root, dirs, files in os.walk('iaprtc12/annotations_complete_eng/'):\n",
    "    for name in files:\n",
    "        annotations.append(os.path.join(root,name))\n",
    "annotations = sorted(annotations)\n",
    "\n",
    "\n",
    "w_f = open(data_file, 'w')\n",
    "# Getting captions \n",
    "for image_idx, image in enumerate(images):\n",
    "    # Check image and annotations file names match\n",
    "    if image.replace('iaprtc12/images/','').strip('.jpg') == \\\n",
    "           annotations[image_idx].replace('iaprtc12/annotations_complete_eng/','')\\\n",
    "           .strip('.eng'):\n",
    "                pass\n",
    "    else:\n",
    "        print('image ',image.replace('iaprtc12/images/','').strip('.jpg'), \n",
    "        'annotation ', annotations[image_idx].replace('iaprtc12/annotations_complete_eng/','')\\\n",
    "           .strip('.eng'))\n",
    "    # Get caption\n",
    "#     print(image_idx)\n",
    "    f = open(annotations[image_idx], 'rb')\n",
    "    for line in f:\n",
    "        line = line.decode(\"latin-1\")\n",
    "        if '<DESCRIPTION>' in line:\n",
    "            # remove unncessary information\n",
    "            line = line.strip('<DESCRIPTION>').replace(';','')\n",
    "            try:\n",
    "                # Some annotations do not follow the same format :@\n",
    "                caption = line[:line.index('</')] \n",
    "            except:\n",
    "                caption = line\n",
    "            break\n",
    "    # Create output line\n",
    "    output_line = image + '*' + caption + '\\n'\n",
    "    # Open output file\n",
    "    w_f.write(output_line)\n",
    "w_f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
